{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 23112), started 1 day, 0:11:32 ago. (Use '!kill 23112' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6be40c0007684146\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6be40c0007684146\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running Tensorboard\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./mylogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2915 images belonging to 5 classes.\n",
      "Found 624 images belonging to 5 classes.\n",
      "Found 628 images belonging to 5 classes.\n",
      "Number Of Images In Training Dataset : 2915\n",
      "Number Of Images In Validation Dataset : 624\n",
      "Number Of Images In Validation Dataset : 628\n",
      "Total number of images: 4167\n"
     ]
    }
   ],
   "source": [
    "# Creating image generators\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_shape = (224, 224, 3) # I want the images to be square and not to big to speed up training.\n",
    "test_split_size = 0.15\n",
    "batch_size = 128\n",
    "rescale_factor = 1/255\n",
    "\n",
    "# Define the parameters for data augmentation\n",
    "train_datagen_2 = ImageDataGenerator(\n",
    "    rescale=rescale_factor,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset_2 = train_datagen_2.flow_from_directory(\n",
    "    'data/flowers_cleaned_split/train',\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_datagen_2 = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create the val dataset\n",
    "val_dataset_2 = val_datagen_2.flow_from_directory(\n",
    "    'data/flowers_cleaned_split/val/',\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_datagen_2 = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset_2 = test_datagen_2.flow_from_directory(\n",
    "    'data/flowers_cleaned_split/test/',\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "print(f'Number Of Images In Training Dataset : {str(train_dataset_2.samples)}')\n",
    "print(f'Number Of Images In Validation Dataset : {str(val_dataset_2.samples)}')\n",
    "print(f'Number Of Images In Validation Dataset : {str(test_dataset_2.samples)}')\n",
    "print(f'Total number of images: {train_dataset_2.samples+val_dataset_2.samples+test_dataset_2.samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def generator(directory_iterator):\n",
    "    for x, y in directory_iterator:\n",
    "        yield x, y\n",
    "\n",
    "train_dataset_tf = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(train_dataset_2),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 224, 224, 3], [None, 3])\n",
    ")\n",
    "\n",
    "val_dataset_tf = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(val_dataset_2),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 224, 224, 3], [None, 3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras.applications import MobileNetV3Large, MobileNetV3Small\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Normalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=1, max_value=3, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=512)\n",
    "    learning_rate = hp.Float(\n",
    "        \"learning_rate\", min_value=1e-4, max_value=1e-2, default=1e-3, sampling=\"log\"\n",
    "    )\n",
    "\n",
    "    base_model = MobileNetV3Large(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    for _ in range(n_hidden):\n",
    "        x = Dense(n_neurons, activation=\"relu\")(x)\n",
    "\n",
    "    outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class MyClassificationHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return build_model(hp)\n",
    "\n",
    "    def fit(self, hp, model, dataset, **kwargs):\n",
    "        # Check if normalization is required and the dataset is a TensorFlow dataset\n",
    "        if hp.Boolean(\"normalize\") and hasattr(dataset, \"map\"):\n",
    "            norm_layer = Normalization()\n",
    "            dataset = dataset.map(lambda x, y: (norm_layer(x), y))\n",
    "        \n",
    "        # Fit the model\n",
    "        return model.fit(dataset, **kwargs)\n",
    "\n",
    "\n",
    "hyperband_tuner = kt.Hyperband(\n",
    "    MyClassificationHyperModel(),\n",
    "    objective=\"val_accuracy\",\n",
    "    seed=42,\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    hyperband_iterations=2,\n",
    "    overwrite=True,\n",
    "    directory=\"hyperparams\",\n",
    "    project_name=\"hyperband\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |2                 |n_hidden\n",
      "35                |35                |n_neurons\n",
      "0.00065625        |0.00065625        |learning_rate\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "2                 |2                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n",
    "tensorboard_cb = TensorBoard(root_logdir)\n",
    "early_stopping_cb = EarlyStopping(patience=2)\n",
    "hyperband_tuner.search(train_dataset_2, epochs=10, validation_data = val_dataset_2,callbacks=[early_stopping_cb, tensorboard_cb] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hej\n"
     ]
    }
   ],
   "source": [
    "# Mobilenet, liten modell\n",
    "from keras.applications import MobileNetV3Large, MobileNetV3Small\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "def create_mobilenet_feature_extraction_model():\n",
    "    base_model = MobileNetV3Large(weights='imagenet', include_top=False)\n",
    "    # Freeze the base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Create a new model on top\n",
    "    inputs = Input(shape=(224, 244, 3))\n",
    "    x = base_model(inputs)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = (Dropout(0.2))(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"hej\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dropout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_mobilenet_feature_extraction_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m, in \u001b[0;36mcreate_mobilenet_feature_extraction_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m GlobalAveragePooling2D()(x)\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m512\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[1;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m (\u001b[43mDropout\u001b[49m(\u001b[38;5;241m0.2\u001b[39m))(x)\n\u001b[0;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs, outputs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dropout' is not defined"
     ]
    }
   ],
   "source": [
    "model2 = create_mobilenet_feature_extraction_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "# Defining tensorboard callback for logging and visualization\n",
    "def get_run_logdir(custom_text=\"\", root_logdir=\"my_logs\"):\n",
    "    formatted_time = strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "    if custom_text:\n",
    "        custom_text = \"_\" + custom_text\n",
    "    return Path(root_logdir) / (formatted_time + custom_text)\n",
    "\n",
    "\n",
    "run_logdir = get_run_logdir(\"transfer_learning_2_mobilenet_Large_averagepooling\")\n",
    "\n",
    "tensorboard_cb = TensorBoard(run_logdir, profile_batch=(100, 200))\n",
    "\n",
    "# Defining earlystopping callback to stop training if model isnt getting better\n",
    "early_stop_cb = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "history = model2.fit(train_dataset_2,epochs=10,steps_per_epoch = steps_per_epoch,\n",
    "                    validation_data = val_dataset_2, validation_steps=validation_steps,\n",
    "                    verbose=True, callbacks=[early_stop_cb, tensorboard_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-learning-lab-O8p5G3Ji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
