{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Transfer learning\n",
    "\n",
    "Jag valde modellen MobileNetV3 för att göra transfer learning. Delvis för att det är intressant att jämföra min egen modells prestanda med en mindre men mer \"professionell\" modell. Dels för att träningen och valet av hyperparametrar ska gå snabbare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Tensorboard\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating image generators\n",
    "# reusing same structure as on my own model, only changing resolution to 224*224\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_shape = (224, 224, 3) # Preferred resolution by Mobilenet\n",
    "test_split_size = 0.15\n",
    "batch_size = 128\n",
    "rescale_factor = 1/255\n",
    "\n",
    "# Define the parameters for data augmentation\n",
    "train_datagen_2 = ImageDataGenerator(\n",
    "    rescale=rescale_factor,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset_2 = train_datagen_2.flow_from_directory(\n",
    "    'data/flowers_cleaned_split/train',\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_datagen_2 = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create the val dataset\n",
    "val_dataset_2 = val_datagen_2.flow_from_directory(\n",
    "    'data/flowers_cleaned_split/val/',\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_datagen_2 = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset_2 = test_datagen_2.flow_from_directory(\n",
    "    'data/flowers_cleaned_split/test/',\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "print(f'Number Of Images In Training Dataset : {str(train_dataset_2.samples)}')\n",
    "print(f'Number Of Images In Validation Dataset : {str(val_dataset_2.samples)}')\n",
    "print(f'Number Of Images In Validation Dataset : {str(test_dataset_2.samples)}')\n",
    "print(f'Total number of images: {train_dataset_2.samples+val_dataset_2.samples+test_dataset_2.samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Generating tf_datasets from the earlier generated datasets because the hyperband wouldnt accept my datasets\n",
    "def generator(directory_iterator):\n",
    "    for x, y in directory_iterator:\n",
    "        yield x, y\n",
    "\n",
    "train_dataset_tf = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(train_dataset_2),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 224, 224, 3], [None, 3])\n",
    ")\n",
    "\n",
    "val_dataset_tf = tf.data.Dataset.from_generator(\n",
    "    lambda: generator(val_dataset_2),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 224, 224, 3], [None, 3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras.applications import MobileNetV3Large\n",
    "from keras import Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Normalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# function to build model, including hyperparameters to be tuned and their respective range\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=1, max_value=3, default=1)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=512)\n",
    "    learning_rate = hp.Float(\n",
    "        \"learning_rate\", min_value=1e-4, max_value=1e-2, default=1e-3, sampling=\"log\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    base_model = MobileNetV3Large(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    for _ in range(n_hidden):\n",
    "        x = Dense(n_neurons, activation=\"relu\")(x)\n",
    "\n",
    "    outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Creating Classificationclass with fit method\n",
    "class MyClassificationHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return build_model(hp)\n",
    "\n",
    "    def fit(self, hp, model, dataset, **kwargs):\n",
    "        # Check if normalization is required and the dataset is a TensorFlow dataset\n",
    "        if hp.Boolean(\"normalize\") and hasattr(dataset, \"map\"):\n",
    "            norm_layer = Normalization()\n",
    "            dataset = dataset.map(lambda x, y: (norm_layer(x), y))\n",
    "        \n",
    "        return model.fit(dataset, **kwargs)\n",
    "\n",
    "# Setting the parameters for the hyperband\n",
    "hyperband_tuner = kt.Hyperband(\n",
    "    MyClassificationHyperModel(),\n",
    "    objective=\"val_accuracy\",\n",
    "    seed=42,\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    hyperband_iterations=2,\n",
    "    overwrite=True,\n",
    "    directory=\"hyperparams\",\n",
    "    project_name=\"hyperband\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "# Running the hyperband and logging it to Tensorboard\n",
    "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n",
    "tensorboard_cb = TensorBoard(root_logdir)\n",
    "early_stopping_cb = EarlyStopping(patience=2)\n",
    "hyperband_tuner.search(train_dataset_2, epochs=10, validation_data = val_dataset_2,callbacks=[early_stopping_cb, tensorboard_cb] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = hyperband_tuner.get_best_models(num_models=1)[0]\n",
    "best_model.save('hyperparams/my_second_best_model.h5')\n",
    "# best val_acc = 0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = hyperband_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hyperparameters.values)\n",
    "\n",
    "# best models parameters:\n",
    "# n_hidden': 1, 'n_neurons': 425, 'learning_rate': 0.0025890919266874407, 'normalize': True,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "steps_per_epoch = train_dataset_2.samples // batch_size\n",
    "validation_steps = val_dataset_2.samples // batch_size\n",
    "history = best_model.fit(train_dataset_2,epochs=10,steps_per_epoch = steps_per_epoch,\n",
    "                    validation_data = val_dataset_2, validation_steps=validation_steps,\n",
    "                    verbose=True, callbacks=[early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Svar på frågor\n",
    "\n",
    "- Motivera ditt modellval \n",
    "  - Jag valde modellen MobileNetV3 för att göra transfer learning. Delvis för att det är intressant att jämföra min egen modells prestanda med en mindre men mer \"professionell\" modell. Dels för att träningen och valet av hyperparametrar ska gå snabbare.\n",
    "- Förklara hur du genomfört transfer learning. \n",
    "  - Jag skapade en base_model där jag valda att inte inkludera det slutgiltiga Denselagrerna, jag valde också att göra de inkluderade lagrerna ej träningsbara.\n",
    "- Förklara hur du systematiskt valt de bästa hyperparametrarna (ska framgå i koden)\n",
    "  - Jag använde Hyperband som är en slags randomiserad \"turnering\" mellan olika värden. Variablerna jag valda att randomisera är:\n",
    "    - Antal lager, mellan 1-3\n",
    "    - Antalet neuroner per lager, mellan 16 och 512\n",
    "    - Learning rate, mellan 0.01 och 0.0001\n",
    "    - Normalisering, True eller False\n",
    "  - Den bästa modellen blev inte särskilt bra, men fick parametrarna: n_hidden': 1, 'n_neurons': 425, 'learning_rate': 0.0025890919266874407, 'normalize': True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-learning-lab-O8p5G3Ji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
